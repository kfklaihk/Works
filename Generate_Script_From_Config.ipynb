{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOOy9x3pbSwrnW3mae3uLKp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kfklaihk/Works/blob/main/Generate_Script_From_Config.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVgd_5Q6AVtJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_hash_config(file_path):\n",
        "    # Read the CSV file (case insensitive headers)\n",
        "    df = pd.read_csv(file_path)\n",
        "    df.columns = df.columns.str.lower()  # Convert headers to lowercase\n",
        "\n",
        "    # Create the four dataframes with updated fields\n",
        "    df1 = df[['table', 'field', 'include_in_select']].drop_duplicates()\n",
        "    df2 = df[['table', 'field', 'translate_after_merge']].drop_duplicates()\n",
        "    df3 = df[['table', 'cross_apply', 'cross_apply_field_name', 'left_join', 'left_join_field_name','left_join_where','hash_method']].drop_duplicates()\n",
        "    df4 = df[['table', 'field', 'hash_method', 'sub_method', 'arg', 'update_after_merge']].drop_duplicates()\n",
        "    df5 = df[['table', 'update_where', 'merge_where']].drop_duplicates()\n",
        "    df6 = df[['table', 'update_after_where']].drop_duplicates()\n",
        "    # Print dataframes for checking\n",
        "    print(\"\\nDataframe 1 (Table, Field):\")\n",
        "    print(df1)\n",
        "    print(\"\\nDataframe 2 (Table, Field, Translate_after_merge):\")\n",
        "    print(df2)\n",
        "    print(\"\\nDataframe 3 (Table, Cross_apply, Cross_apply_Field_Name, Left_join, Left_join_Field_Name,Left_join_where, Hash_Method):\")\n",
        "    print(df3)\n",
        "    print(\"\\nDataframe 4 (Table, Field, Hash_Method, Sub_Method, Arg, Update_after_merge):\")\n",
        "    print(df4)\n",
        "    print(\"\\nDataframe 5 (Table, Update_where, Merge_where):\")\n",
        "    print(df5)\n",
        "    print(\"\\nDataframe 6 (Table, Update_after_where):\")\n",
        "    print(df6)\n",
        "    # Process each unique table\n",
        "    unique_tables = df1['table'].unique()\n",
        "    list1 = []\n",
        "    list1_1=[]\n",
        "    list2 = []\n",
        "    list3 = []\n",
        "    list4 = []\n",
        "    list5 = []\n",
        "    list6 = []      # New list for update_after_merge = Y\n",
        "    list7 = []\n",
        "    list8 = []\n",
        "    list9 = []\n",
        "    list11 = []\n",
        "\n",
        "    for table in unique_tables:\n",
        "        # List 1: Modified to check Include_in_Select\n",
        "        fields = df1[(df1['table'] == table) & (df1['include_in_select'] == 'Y')]['field'].apply(lambda x: f\"org.{x}\").tolist()\n",
        "        cross_apply_fields = df3[df3['table'] == table]['cross_apply_field_name'].dropna().unique().tolist()\n",
        "        left_join_fields = df3[df3['table'] == table]['left_join_field_name'].dropna().unique().tolist()\n",
        "        all_fields = ['org.ID'] + fields + cross_apply_fields + left_join_fields\n",
        "       # all_fields_1 = left_join_fields\n",
        "        list1.append((table, all_fields))\n",
        "       # list1_1.append((table, all_fields_1))\n",
        "\n",
        "        # List 2: Create tuples with table and constructed SQL\n",
        "        table_df3 = df3[df3['table'] == table]\n",
        "        sql_texts = []\n",
        "        # Group by cross_apply_field_name and hash_method\n",
        "        grouped = table_df3.groupby(['cross_apply_field_name', 'hash_method'])\n",
        "\n",
        "        for (field_name, hash_method), group in grouped:\n",
        "\n",
        "            if hash_method == 'fn_dc_method11_hash':\n",
        "                # Concatenate all cross_apply fields with ||\n",
        "                cross_applies = [f\"org.{x}\" for x in group['cross_apply']]\n",
        "                concatenated = \" || \".join(cross_applies)\n",
        "                s1 = f\"CONCAT(m11, {concatenated})\"\n",
        "                sql = f\"SELECT fn_dc_method11_hash({s1}, m11) {field_name} FROM dual\"\n",
        "                sql_texts.append(sql)\n",
        "        if sql_texts:\n",
        "            list2.append((table, sql_texts))\n",
        "        # Group by left_join_field_name and hash_method\n",
        "        sql_texts = []\n",
        "        grouped_left = table_df3.groupby(['left_join_field_name', 'hash_method'])\n",
        "\n",
        "        for (field_name, hash_method), group in grouped_left:\n",
        "            if hash_method == 'Method5':\n",
        "                # For Method5, process each left_join separately\n",
        "                for index, (i,row) in enumerate(group.iterrows()):\n",
        "                    left_join = f\"org.{row['left_join']}\"\n",
        "                    left_join_where = f\"org.{row['left_join_where']}\"\n",
        "                    left_join_where_flag = f\"and {left_join_where}\" if pd.notna(row['left_join_where']) else None\n",
        "                    sql = f\"\"\"SELECT map.randID {field_name}\n",
        "                    FROM dm_mapping_list_hash map\n",
        "                    WHERE SUBSTR({left_join}, -6) = map.sourceID \\n            {left_join_where_flag if left_join_where_flag else ''}\"\"\"\n",
        "                    sql_texts.append(sql)\n",
        "\n",
        "        if sql_texts:\n",
        "            list3.append((table, sql_texts))\n",
        "\n",
        "        #if sql_texts2:\n",
        "        #    list3.append((table, sql_texts2))\n",
        "\n",
        "        # List 4: Modified to check Update_after_Merge is blank\n",
        "        table_df4 = df4[(df4['table'] == table) & (df4['update_after_merge'].isna())]\n",
        "        sql_texts4 = []\n",
        "\n",
        "        for _, row in table_df4.iterrows():\n",
        "            if table_df3[table_df3['table']==table].drop(columns=['hash_method']).drop(columns=['table']).isna().all().all():\n",
        "                field = f\"dst.{row['field']}\"\n",
        "                arg = f\"dst.{row['arg']}\" if pd.notna(row['arg']) else None\n",
        "            else:\n",
        "                field = f\"dst.{row['field']}\"\n",
        "                arg = f\"src.{row['arg']}\" if pd.notna(row['arg']) else None\n",
        "\n",
        "\n",
        "\n",
        "            if row['hash_method'] == 'fn_dc_method11_hash':\n",
        "                if row['sub_method'] == 'method1':\n",
        "                    sql = f\"{field} = CASE WHEN {field} IS NOT NULL THEN SUBSTR({arg}, 1, 8) ELSE NULL END\"\n",
        "                elif row['sub_method'] == 'method2':\n",
        "                    sql = f\"{field} = CASE WHEN {field} IS NOT NULL THEN SUBSTR({arg}, 9, 8) ELSE NULL END\"\n",
        "                elif row['sub_method'] == 'method3':\n",
        "                    sql = f\"{field} = CASE WHEN {field} IS NULL THEN NULL ELSE SUBSTR({arg}, 1, 8) || ' ' || SUBSTR({arg}, 9, 8) END\"\n",
        "                elif row['sub_method'] == 'method4':\n",
        "                    sql = f\"{field} = CASE WHEN {field} IS NULL THEN NULL ELSE SUBSTR({arg}, 1, 16) END\"\n",
        "\n",
        "            elif row['hash_method'] == 'Method5':\n",
        "                sql = f\"{field} = CASE WHEN {field} IS NULL THEN NULL WHEN {field} in ('NOT PROVIDED' , 'N/A') THEN {field} ELSE CONCAT(SUBSTR({field}, 1, LENGTH({field}) - LENGTH(SUBSTR({field}, -6))), COALESCE({arg}, SUBSTR({field}, -6))) END\"\n",
        "\n",
        "            elif row['hash_method'] == 'fn_dc_method10_passport_hash':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"\"\"{field} = CASE WHEN {field} IS NULL THEN NULL\n",
        "                                        WHEN {field} in ('NOT PROVIDED' , 'N/A') THEN cast({field} as nvarchar2(100))\n",
        "                                        ELSE fn_dc_method10_passport_hash({field})\n",
        "                                        END\"\"\"\n",
        "                elif row['sub_method'] == 'method1':\n",
        "                    sql = f\"\"\"{field} = CASE\n",
        "                        WHEN {field} like '%-PASSPORT-%' THEN CONCAT(substr({field},1, instr({field},'-PASSPORT-')+9) , fn_dc_method10_passport_hash(substr({field},instr({field},'-PASSPORT-')+10,99)))\n",
        "                        WHEN SUBSTR({field}, 1, 9) = 'PASSPORT-' THEN CONCAT(SUBSTR({field}, 1, 9), fn_dc_method10_passport_hash(SUBSTR({field}, 10, LENGTH({field})-9)))\n",
        "                        WHEN SUBSTR({field}, 1, 5) = 'HKID-' THEN CONCAT(SUBSTR({field}, 1, 5), fn_dc_method10_passport_hash(SUBSTR({field}, 6, LENGTH({field})-5)))\n",
        "                        WHEN {field} like '%HKID-%' THEN substr({field},0, instr({field},'HKID-')+4) || rtrim(fn_dc_method10_passport_hash(substr({field}, instr({field},'HKID-')+5,6)))||\n",
        "                            substr(use_customer_no, instr({field},'HKID-')+11,99)\n",
        "                        WHEN {field} in ('NOT PROVIDED' , 'N/A') THEN cast({field} as nvarchar2(100))\n",
        "                        ELSE fn_dc_method10_passport_hash({field})\n",
        "                        END\"\"\"\n",
        "                elif row['sub_method'] == 'method2':\n",
        "                    sql = f\"\"\"{field} = CASE\n",
        "                        WHEN {field} like '%-PASSPORT-%' THEN CONCAT(substr({field},1, instr({field},'-PASSPORT-')+9) , fn_dc_method10_passport_hash(substr({field},instr({field},'-PASSPORT-')+10,99)))\n",
        "                        WHEN SUBSTR({field}, 1, 9) = 'PASSPORT-' THEN CONCAT(SUBSTR({field}, 1, 9), fn_dc_method10_passport_hash(SUBSTR({field}, 10, LENGTH({field})-9)))\n",
        "                        WHEN SUBSTR({field}, 1, 5) = 'HKID-' THEN CONCAT(SUBSTR({field}, 1, 5), fn_dc_method10_passport_hash(SUBSTR({field}, 6, LENGTH({field})-5)))\n",
        "                        WHEN {field} like '%HKID-%' THEN substr({field},0, instr({field},'HKID-')+4) || rtrim(fn_dc_method10_passport_hash(substr({field}, instr({field},'HKID-')+5,6)))||\n",
        "                        substr(use_customer_no, instr({field},'HKID-')+11,99)\n",
        "                        WHEN {field} like '%BR-%' Then substr({field}, 0,  instr({field},'BR-')+2)||rtrim(fn_dc_method10_passport_hash(substr({field},instr({field},'BR-')+3,99)))\n",
        "\t\t\t            WHEN {field} like '%OTHERS-%' Then substr({field}, 0,  instr({field},'OTHERS-')+6)||rtrim(fn_dc_method10_passport_hash(substr({field},instr({field},'OTHERS-')+7,99)))\n",
        "\t\t\t            WHEN {field} like '%SO-%' Then substr({field}, 0,  instr({field},'SO-')+2)||rtrim(fn_dc_method10_passport_hash(substr({field},instr({field},'SO-')+3,99)))\n",
        "\t\t                WHEN {field} like '%IRD-%' Then substr({field}, 0,  instr({field},'IRD-')+3)||rtrim(fn_dc_method10_passport_hash(substr({field},instr({field},'IRD-')+4,99)))\n",
        "                        WHEN {field} in ('NOT PROVIDED' , 'N/A') THEN cast({field} as nvarchar2(100))\n",
        "                        ELSE fn_dc_method10_passport_hash({field})\n",
        "                        END\"\"\"\n",
        "                elif row['sub_method'] == 'method3':\n",
        "                    sql = f\"\"\"{field} = CASE\n",
        "\t\t                WHEN {field} in ('NOT PROVIDED' , 'N/A') THEN {field}\n",
        "\t\t                WHEN dst.av_id_type_code = 'PASSPORT' THEN to_char(fn_dc_method10_passport_hash({field})    )\n",
        "\t\t                ELSE CONCAT(SUBSTR({field}, 1, LENGTH({field}) - LENGTH(SUBSTR({field}, -6))), COALESCE(src.id_mapping, SUBSTR({field}, -6)))\n",
        "\t\t                END\"\"\"\n",
        "\n",
        "            elif row['hash_method'] == 'fn_dc_method14_hash':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = CASE WHEN UPPER({field}) not in ('NOT PROVIDED' , 'N/A') THEN fn_dc_method14_hash({field}, m14s , m14t) ELSE cast({field} as nvarchar2(100)) END\"\n",
        "                elif row['sub_method'] == 'method1':\n",
        "                    sql = f\"{field} = CASE WHEN UPPER({field}) not in ('NOT PROVIDED' , 'N/A') THEN REGEXP_REPLACE(fn_dc_method14_hash({field}, m14s , m14t), '[^0-9a-zA-Z]','') ELSE cast({field}  as nvarchar2(100)) END\"\n",
        "\n",
        "            elif row['hash_method'] == 'fn_dc_method12_hash':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = fn_dc_method12_hash({field}, m12)\"\n",
        "\n",
        "            elif row['hash_method'] == 'fn_dc_method16_hash':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = fn_dc_method16_hash()\"\n",
        "\n",
        "            elif row['hash_method'] == 'fn_dc_method3_hash':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = fn_dc_method3_hash({field}, m03)\"\n",
        "\n",
        "            elif row['hash_method'] == 'genCheckDigit_hash':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = CASE WHEN {arg} IS NOT NULL THEN genCheckDigit_hash({arg}) END\"\n",
        "                elif row['sub_method'] == 'HKID':\n",
        "                    sql = f\"{field} = CASE WHEN dst.av_id_type_code='HKID' and {arg} IS NOT NULL  THEN genCheckDigit_hash({arg}) END\"\n",
        "\n",
        "\n",
        "            elif row['hash_method'] == 'Method2':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = CASE WHEN {arg} IS NOT NULL THEN hashNumeric_hash({arg}, 0, m02) END\"\n",
        "                elif row['sub_method'] == 'method1':\n",
        "                    sql = f\"{field} = TO_NUMBER(hashNumeric_hash(TO_CHAR({arg}), 0, m02))\"\n",
        "\n",
        "            elif row['hash_method'] == 'Method17':\n",
        "                sql = f\"{field} = '{arg[4:]}'\"\n",
        "            elif row['hash_method'] == 'fn_dc_method13_hash':\n",
        "                sql = f\"{field} = fn_dc_method13_hash()\"\n",
        "\n",
        "            elif row['hash_method'] == 'fn_dc_method15_hash':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = CASE WHEN {field} is null then null else fn_dc_method15_hash() END\"\n",
        "            sql_texts4.append(sql)\n",
        "\n",
        "        if sql_texts4:\n",
        "            list4.append((table, sql_texts4))\n",
        "\n",
        "        # List 5: Updated to use translate_after_merge\n",
        "        table_df2 = df2[df2['table'] == table]\n",
        "        sql_texts5 = []\n",
        "\n",
        "        for _, row in table_df2.iterrows():\n",
        "            field = f\"{row['field']}\"\n",
        "\n",
        "            if row['translate_after_merge'] == 'Y':\n",
        "                sql = f\"dst.{field} = translate(dst.{field}, eng_translate_src, eng_translate_tgt)\"\n",
        "            elif row['translate_after_merge'] == 'Y1':\n",
        "                sql = f\"dst.{field} = translate(substr(dst.{field},1,1), zhhk_translate_src, zhhk_translate_tgt)\"\n",
        "            elif row['translate_after_merge'] == 'Y2':\n",
        "                sql = f\"dst.{field} = translate(substr(dst.{field},2,2), zhhk_translate_src, zhhk_translate_tgt)\"\n",
        "            elif row['translate_after_merge'] == 'Y3':\n",
        "                sql = f\"dst.{field} = translate(substr(dst.{field},1,10), zhhk_translate_src, zhhk_translate_tgt)\"\n",
        "            if row['translate_after_merge'] in ['Y', 'Y1', 'Y2', 'Y3']:\n",
        "                sql_texts5.append(sql)\n",
        "\n",
        "        if sql_texts5:\n",
        "            list5.append((table, sql_texts5))\n",
        "\n",
        "        # List 6: New list for Update_after_Merge = Y\n",
        "        table_df6 = df4[(df4['table'] == table) & (df4['update_after_merge'] == 'Y')]\n",
        "        sql_texts6 = []\n",
        "        sql_texts11 = []\n",
        "\n",
        "        for _, row in table_df6.iterrows():\n",
        "            field = f\"dst.{row['field']}\"\n",
        "            arg = f\"dst.{row['arg']}\" if pd.notna(row['arg']) else None\n",
        "            arg1 = f\"src.{row['arg']}\" if pd.notna(row['arg']) else None\n",
        "\n",
        "            if row['hash_method'] == 'genCheckDigit_hash':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = CASE WHEN {arg} IS NOT NULL THEN genCheckDigit_hash({arg}) END\"\n",
        "                elif row['sub_method'] == 'HKID':\n",
        "                    sql = f\"{field} = CASE WHEN av_id_type_code='HKID' AND  (LENGTH({arg})  =7 or LENGTH({arg})  =8) THEN genCheckDigit_hash({arg}) END\"\n",
        "                sql_texts6.append(sql)\n",
        "            elif row['hash_method'] == 'fn_dc_method10_passport_hash':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = CASE WHEN {field} IS NOT NULL AND {field} <> 'N/A' THEN fn_dc_method10_passport_hash({arg}) END\"\n",
        "                elif row['sub_method'] == 'method1':\n",
        "                    sql = f\"\"\"{field} = CASE\n",
        "                        WHEN {field} like '%-PASSPORT-%' THEN CONCAT(substr({field},1, instr({field},'-PASSPORT-')+9) , fn_dc_method10_passport_hash(substr({field},instr({field},'-PASSPORT-')+10,99)))\n",
        "                        WHEN SUBSTR({field}, 1, 9) = 'PASSPORT-' THEN CONCAT(SUBSTR({field}, 1, 9), fn_dc_method10_passport_hash(SUBSTR({field}, 10, LENGTH({field})-9)))\n",
        "                        WHEN SUBSTR({field}, 1, 5) = 'HKID-' THEN CONCAT(SUBSTR({field}, 1, 5), fn_dc_method10_passport_hash(SUBSTR({field}, 6, LENGTH({field})-5)))\n",
        "                        WHEN {field} like '%HKID-%' THEN substr({field},0, instr({field},'HKID-')+4) || rtrim(fn_dc_method10_passport_hash(substr({field}, instr({field},'HKID-')+5,6)))||\n",
        "                            substr(use_customer_no, instr({field},'HKID-')+11,99)\n",
        "                        WHEN {field} in ('NOT PROVIDED' , 'N/A') THEN cast({field} as nvarchar2(100))\n",
        "                        ELSE fn_dc_method10_passport_hash({field})\n",
        "                        END\"\"\"\n",
        "                elif row['sub_method'] == 'method2':\n",
        "                    sql = f\"\"\"{field} = CASE\n",
        "\t\t\t\t    WHEN {field} in ('NOT PROVIDED' , 'N/A') THEN cast({field} as nvarchar2(100)) \t\t\t\t\t\t\t\t\t\tWHEN av_id_type_code='PASSPORT' THEN fn_dc_method10_passport_hash({field})\n",
        "\t\t\t\t    WHEN av_id_type_code <> 'HKID' AND \tav_id_type_code <> 'PASSPORT' THEN fn_dc_method14_hash({field}, m14s , m14t)\n",
        "\t\t\t\t    WHEN av_id_type_code = 'HKID' THEN TO_NCHAR({field})\n",
        "\t\t\t\t    ELSE fn_dc_method10_passport_hash({field})\n",
        "\t\t\t        END\"\"\"\n",
        "                sql_texts6.append(sql)\n",
        "            elif row['hash_method'] == 'fn_dc_method16_hash':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = fn_dc_method16_hash()\"\n",
        "                sql_texts6.append(sql)\n",
        "            elif row['hash_method'] == 'Method17':\n",
        "                sql = f\"{field} = '{arg[4:]}'\"\n",
        "                sql_texts6.append(sql)\n",
        "            elif row['hash_method'] == 'fn_dc_method3_hash':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = fn_dc_method3_hash({arg}, m03)\"\n",
        "                sql_texts6.append(sql)\n",
        "            elif row['hash_method'] == 'fn_dc_method14_hash':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = CASE WHEN UPPER({field}) not in ('NOT PROVIDED' , 'N/A') THEN fn_dc_method14_hash({arg}, m14s , m14t) ELSE cast({arg} as nvarchar2(100)) END\"\n",
        "                sql_texts6.append(sql)\n",
        "            elif row['hash_method'] == 'fn_dc_method12_hash':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = fn_dc_method12_hash({field}, m12)\"\n",
        "                sql_texts6.append(sql)\n",
        "            elif row['hash_method'] == 'Special':\n",
        "                if pd.isna(row['sub_method']):\n",
        "                    sql = f\"{field} = substr({arg1},-3,3)\"\n",
        "                sql_texts11.append(sql)\n",
        "            else:\n",
        "                sql_texts6.append(f\"\"\"{field} = {row['hash_method']}({arg})\"\"\")\n",
        "\n",
        "\n",
        "        if sql_texts6:\n",
        "            list6.append((table, sql_texts6))\n",
        "        if sql_texts11:\n",
        "            list11.append((table, sql_texts11))\n",
        "        # List 7 & 8: Get update_where and merge_where conditions\n",
        "        table_df5 = df5[df5['table'] == table]\n",
        "\n",
        "        # List 7: Update where conditions\n",
        "        update_where_conditions = table_df5['update_where'].dropna().tolist()\n",
        "        if update_where_conditions:\n",
        "            list7.append((table, update_where_conditions))\n",
        "\n",
        "        # List 8: Merge where conditions\n",
        "        merge_where_conditions = table_df5['merge_where'].dropna().tolist()\n",
        "        if merge_where_conditions:\n",
        "            list8.append((table, merge_where_conditions))\n",
        "\n",
        "        # List 9: Update after where conditions\n",
        "        table_df6 = df6[df6['table'] == table]\n",
        "        update_after_where_conditions = table_df6['update_after_where'].dropna().tolist()\n",
        "        if update_after_where_conditions:\n",
        "            list9.append((table, update_after_where_conditions))\n",
        "\n",
        "    # Print the resulting lists\n",
        "    print(\"\\nList 1 (Table with Fields):\")\n",
        "    for item in list1:\n",
        "        print(f\"\\nTable: {item[0]}\")\n",
        "        print(\"Fields:\", item[1])\n",
        "\n",
        "    print(\"\\nList 2 (Table with SQL):\")\n",
        "    for item in list2:\n",
        "        print(f\"\\nTable: {item[0]}\")\n",
        "        print(\"SQL statements:\")\n",
        "        for sql in item[1]:\n",
        "            print(sql)\n",
        "            print()\n",
        "\n",
        "    print(\"\\nList 3 (Table with SQL):\")\n",
        "    for item in list3:\n",
        "        print(f\"\\nTable: {item[0]}\")\n",
        "        print(\"SQL statements:\")\n",
        "        for sql in item[1]:\n",
        "            print(sql)\n",
        "            print()\n",
        "\n",
        "    print(\"\\nList 4 (Table with SQL):\")\n",
        "    for item in list4:\n",
        "        print(f\"\\nTable: {item[0]}\")\n",
        "        print(\"SQL statements:\")\n",
        "        # Create a dictionary to count occurrences\n",
        "        for sql in item[1]:\n",
        "            print(sql)\n",
        "            print()\n",
        "\n",
        "    print(\"\\nList 5 (Table with Translation SQL):\")\n",
        "    for item in list5:\n",
        "        print(f\"\\nTable: {item[0]}\")\n",
        "        print(\"SQL statements:\")\n",
        "        for sql in item[1]:\n",
        "            print(sql)\n",
        "            print()\n",
        "\n",
        "    print(\"\\nList 6 (Table with Translation SQL):\")\n",
        "    for item in list6:\n",
        "        print(f\"\\nTable: {item[0]}\")\n",
        "        print(\"SQL statements:\")\n",
        "        for sql in item[1]:\n",
        "            print(sql)\n",
        "            print()\n",
        "\n",
        "    print(\"\\nList 7 (Table with SQL):\")\n",
        "    for item in list7:\n",
        "        print(f\"\\nTable: {item[0]}\")\n",
        "        print(\"Update where conditions:\")\n",
        "        for sql in item[1]:\n",
        "            print(sql)\n",
        "\n",
        "\n",
        "    print(\"\\nList 8 (Table with Where Conditions):\")\n",
        "    for item in list8:\n",
        "        print(f\"\\nTable: {item[0]}\")\n",
        "        print(\"Merge where conditions:\")\n",
        "        for condition in item[1]:\n",
        "            print(condition)\n",
        "\n",
        "    print(\"\\nList 9 (Table with Where Conditions):\")\n",
        "    for item in list9:\n",
        "        print(f\"\\nTable: {item[0]}\")\n",
        "        print(\"Update after where conditions:\")\n",
        "        for condition in item[1]:\n",
        "            print(condition)\n",
        "\n",
        "    print(\"\\nList 11 (Table with SQL):\")\n",
        "    for item in list11:\n",
        "        print(f\"\\nTable: {item[0]}\")\n",
        "        print(\"SQL statements:\")\n",
        "        for sql in item[1]:\n",
        "            print(sql)\n",
        "\n",
        "    def generate_final_sql(list1, list2, list3, list4, list5, list6, list7, list8, list9):\n",
        "        final_list = []\n",
        "\n",
        "\n",
        "\n",
        "        # Process each table\n",
        "        for table in df1['table'].unique():\n",
        "            sql_parts = []\n",
        "            cross_apply_list = next((item[1] for item in list2 if item[0] == table), [])\n",
        "            left_join_list = next((item[1] for item in list3 if item[0] == table), [])\n",
        "            if cross_apply_list or left_join_list:\n",
        "            # Section 1\n",
        "                fields_list = next((item[1] for item in list1 if item[0] == table), [])\n",
        "                section1 = f\"\"\"\n",
        "                ---------------\n",
        "                -- {table}\n",
        "                ---------------\n",
        "                dbms_output.put_line('-- hashing {table} --');\n",
        "                MERGE INTO {table} dst\n",
        "                USING (\n",
        "                    SELECT\n",
        "                        {',\\n                '.join(fields_list)}\n",
        "                    FROM {table} org\"\"\"\n",
        "                sql_parts.append(section1)\n",
        "                if cross_apply_list:\n",
        "                    section2 = '\\n'.join(f\"\"\"\n",
        "                    CROSS APPLY (\n",
        "                        {ca}\n",
        "                    )\"\"\" for ca in cross_apply_list)\n",
        "                    sql_parts.append(section2)\n",
        "                if left_join_list:\n",
        "                    section2_1 = '\\n'.join(f\"\"\"\n",
        "                    OUTER APPLY (\n",
        "                        {lf}\n",
        "                    )\"\"\" for lf in left_join_list)\n",
        "                    sql_parts.append(section2_1)\n",
        "\n",
        "            # Section 3\n",
        "          #  where_conditions = next((item[1] for item in list3 if item[0] == table), [])\n",
        "                section3 = \"\"\"\n",
        "                    WHERE org.SCHEME_CODE = schemeAbbreviation\n",
        "                    \"\"\" + '\\n                    '.join([f\"AND {condition}\" for condition in next((item[1] for item in list8 if item[0] == table), [])]) + f\"\"\"\n",
        "                ) src\n",
        "                ON (dst.ID = src.ID)\n",
        "                WHEN MATCHED THEN UPDATE SET\"\"\"\n",
        "                sql_parts.append(section3)\n",
        "                update_statements = next((item[1] for item in list4 if item[0] == table), [])\n",
        "                section4 = f\"\"\"            {',\\n            '.join(update_statements)};\n",
        "                recCount := SQL%rowcount;\n",
        "                dbms_output.put_line('.  {table}(id, name)=' || recCount);\"\"\"\n",
        "                sql_parts.append(section4)\n",
        "            else:\n",
        "                section0 = f\"\"\"\n",
        "                ---------------\n",
        "                -- {table}\n",
        "                ---------------\n",
        "                dbms_output.put_line('-- hashing {table} --');\n",
        "                UPDATE {table} dst\n",
        "                SET\n",
        "                \"\"\"\n",
        "                sql_parts.append(section0)\n",
        "                update_statements = next((item[1] for item in list4 if item[0] == table), [])\n",
        "                section4 = f\"\"\"            {',\\n            '.join(update_statements)}\n",
        "                WHERE SCHEME_CODE = schemeAbbreviation \"\"\" + '\\n            '.join([f\"AND {condition}\" for condition in next((item[1] for item in list7 if item[0] == table), [])]) + f\"\"\";\n",
        "                recCount := SQL%rowcount;\n",
        "                dbms_output.put_line('.  {table}(id, name)=' || recCount);\"\"\"\n",
        "                sql_parts.append(section4)\n",
        "\n",
        "            # Section 5 (optional)\n",
        "            translate_statements = next((item[1] for item in list5 if item[0] == table), [])\n",
        "            if translate_statements:\n",
        "                section5 = f\"\"\"\n",
        "        UPDATE {table} dst\n",
        "        SET\n",
        "            {',\\n            '.join(translate_statements)}\n",
        "        WHERE SCHEME_CODE = schemeAbbreviation;\n",
        "\n",
        "        recCount := SQL%rowcount;\n",
        "        dbms_output.put_line('.  {table}(translate)=' || recCount);\"\"\"\n",
        "                sql_parts.append(section5)\n",
        "\n",
        "            update_statements = next((item[1] for item in list6 if item[0] == table), [])\n",
        "            if update_statements:\n",
        "                section6 = f\"\"\"\n",
        "        UPDATE {table} dst\n",
        "        SET\n",
        "            {',\\n        '.join(update_statements)}\n",
        "        WHERE SCHEME_CODE = schemeAbbreviation \"\"\" + '\\n            '.join([f\"AND {condition}\" for condition in next((item[1] for item in list9 if item[0] == table), [])]) + f\"\"\";\n",
        "        recCount:=SQL%rowcount;\n",
        "        dbms_output.put_line('.  {table}(update)=' || recCount);\"\"\"\n",
        "                sql_parts.append(section6)\n",
        "\n",
        "            special_update_statements = next((item[1] for item in list11 if item[0] == table), [])\n",
        "            if special_update_statements:\n",
        "                section11 = f\"\"\"\n",
        "        MERGE INTO {table} dst\n",
        "        USING\n",
        "\t\t\t(\n",
        "\t\t\t\tSELECT\n",
        "\t\t\t\tdoc.client_uuid,\n",
        "\t\t\t\tdoc.ID_no\n",
        "\t\t\t\tFROM\n",
        "\t\t\t\tCMN_CLIENT_ID_DOC doc\n",
        "\t\t\t\tLEFT JOIN\n",
        "\t\t\t\tREG_EMPLOYER er on (doc.client_uuid = er.id)\n",
        "\t\t\t\tWHERE\n",
        "\t\t\t\t\tdoc.AV_ID_TYPE_CODE ='BR' and doc.ID_NO IS NOT NULL\n",
        "\t\t\t\t) src\n",
        "\t\t\tON\n",
        "\t\t\t(\n",
        "\t\t\t\tsrc.client_uuid = dst.ID\n",
        "\t\t\t)\n",
        "\t\t\tWHEN MATCHED THEN UPDATE\n",
        "\t\t\tSET\n",
        "            {',\\n        '.join(special_update_statements)};\n",
        "        recCount:=SQL%rowcount;\n",
        "        dbms_output.put_line('.  {table}(update)=' || recCount);\"\"\"\n",
        "                sql_parts.append(section11)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "            # Combine all sections\n",
        "            final_sql = '\\n'.join(sql_parts)\n",
        "            final_list.append((table, final_sql))\n",
        "\n",
        "\n",
        "\n",
        "        # Print the final generated SQL for each table\n",
        "        print(\"\\nFinal Generated SQL:\")\n",
        "\n",
        "        # Export to file\n",
        "        with open('hashing.sql', 'w') as f:\n",
        "            # Write header with consistent indentation\n",
        "            f.write(\"\"\"DECLARE\n",
        "    recCount            NUMBER;\n",
        "    schemeAbbreviation VARCHAR2(10)  := 'XXXX';\n",
        "    m02                VARCHAR2(100) := 'XXXX';\n",
        "    m03                VARCHAR2(100) := 'XXXX';\n",
        "    m11                VARCHAR2(100) := 'XXXX';\n",
        "    m12                VARCHAR2(100) := 'XXXX';\n",
        "    m14s               VARCHAR2(100) := 'XXXX';\n",
        "    m14t               VARCHAR2(100) := 'XXXX';\n",
        "    eng_translate_src  VARCHAR2(100) := 'XXXX';\n",
        "    eng_translate_tgt  VARCHAR2(100) := 'XXXX';\n",
        "    zhhk_translate_src VARCHAR2(100) := 'XXXX';\n",
        "    zhhk_translate_tgt VARCHAR2(100) := 'XXXX';\n",
        "BEGIN\"\"\")\n",
        "\n",
        "            for table, sql in final_list:\n",
        "                # Add newlines between each table's SQL block\n",
        "                f.write(f\"\\n\\n{sql}\")\n",
        "\n",
        "            f.write(\"\"\"\n",
        "\n",
        "END;\n",
        "/\"\"\")\n",
        "\n",
        "        return final_list\n",
        "\n",
        "    # Add this call after all lists are created\n",
        "    final_sql_list = generate_final_sql(list1, list2, list3, list4, list5, list6, list7,list8,list9)\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    file_path = \"Sample1.csv\"\n",
        "    process_hash_config(file_path)\n"
      ]
    }
  ]
}